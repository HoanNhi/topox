{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Hypergraph Message Passing Neural Network (HMPNN)\n",
    "\n",
    "In this notebook, we will create and train a Hypergraph Message Passing Neural Network in the hypergraph domain. This method is introduced in the paper [Message Passing Neural Networks for\n",
    "Hypergraphs](https://arxiv.org/abs/2203.16995) by Heydari et Livi 2022. We will use a benchmark dataset, Cora, a collection of 2708 academic papers and 5429 citation relations, to do the task of node classification. There are 7 category labels, namely `Case_Based`, `Genetic_Algorithms`, `Neural_Networks`, `Probabilistic_Methods`, `Reinforcement_Learning`, `Rule_Learning` and `Theory`.\n",
    "\n",
    "Each document is initially represented as a binary vector of length 1433, standing for a unique subset of the words within the papers, in which a value of 1 means the presence of its corresponding word in the paper."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:14:58.423154Z",
     "start_time": "2025-11-17T22:14:53.129067Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch_geometric.datasets as geom_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from topomodelx.nn.hypergraph.hmpnn import HMPNN\n",
    "\n",
    "torch.manual_seed(0)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intel500g/thomas/miniconda3/envs/topox/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7dab70097af0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU's are available, we will make use of them. Otherwise, this will run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:14:58.460608Z",
     "start_time": "2025-11-17T22:14:58.455828Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we download the dataset. It contains initial representation of nodes, the adjacency information, category labels and train-val-test masks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:14:58.655104Z",
     "start_time": "2025-11-17T22:14:58.616730Z"
    }
   },
   "source": [
    "dataset = geom_datasets.Planetoid(root=\"tmp/\", name=\"cora\")[0]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:14:58.926398Z",
     "start_time": "2025-11-17T22:14:58.915730Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.num_nodes",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct the incidence matrix ($B_1$) which is of shape $n_\\text{nodes} \\times n_\\text{edges}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:14:59.668772Z",
     "start_time": "2025-11-17T22:14:59.283739Z"
    }
   },
   "source": [
    "dataset[\"incidence_1\"] = torch.sparse_coo_tensor(\n",
    "    dataset[\"edge_index\"], torch.ones(dataset[\"edge_index\"].shape[1]), dtype=torch.long\n",
    ")\n",
    "dataset = dataset.to(device)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:14:59.713411Z",
     "start_time": "2025-11-17T22:14:59.689250Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.node_stores[0]['train_mask']",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:14:59.894926Z",
     "start_time": "2025-11-17T22:14:59.890195Z"
    }
   },
   "source": [
    "x_0s = dataset[\"x\"]\n",
    "y = dataset[\"y\"]\n",
    "incidence_1 = dataset[\"incidence_1\"]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "We then specify the hyperparameters and construct the model, the loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:14:59.935657Z",
     "start_time": "2025-11-17T22:14:59.920865Z"
    }
   },
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \"\"\"Network class that initializes the base model and readout layer.\n",
    "\n",
    "    Base model parameters:\n",
    "    ----------\n",
    "    Reqired:\n",
    "    in_channels : int\n",
    "        Dimension of the input features.\n",
    "    hidden_channels : int\n",
    "        Dimension of the hidden features.\n",
    "\n",
    "    Optitional:\n",
    "    **kwargs : dict\n",
    "        Additional arguments for the base model.\n",
    "\n",
    "    Readout layer parameters:\n",
    "    ----------\n",
    "    out_channels : int\n",
    "        Dimension of the output features.\n",
    "    task_level : str\n",
    "        Level of the task. Either \"graph\" or \"node\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, hidden_channels, out_channels, task_level=\"graph\", **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the model\n",
    "        self.base_model = HMPNN(\n",
    "            in_channels=in_channels, hidden_channels=hidden_channels, **kwargs\n",
    "        )\n",
    "\n",
    "        # Readout\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.out_pool = task_level == \"graph\"\n",
    "\n",
    "    def forward(self, x_0, x_1, incidence_1):\n",
    "        # Base model\n",
    "        x_0, x_1 = self.base_model(x_0, x_1, incidence_1)\n",
    "\n",
    "        # Pool over all nodes in the hypergraph\n",
    "        x = torch.max(x_0, dim=0)[0] if self.out_pool is True else x_0\n",
    "\n",
    "        return self.linear(x)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:15:00.053632Z",
     "start_time": "2025-11-17T22:14:59.993319Z"
    }
   },
   "cell_type": "code",
   "source": "torch.unique(y).shape[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:15:00.270383Z",
     "start_time": "2025-11-17T22:15:00.247099Z"
    }
   },
   "source": [
    "# Base model hyperparameters\n",
    "in_channels = x_0s.shape[1]\n",
    "hidden_channels = 128\n",
    "n_layers = 1\n",
    "\n",
    "# Readout hyperparameters\n",
    "out_channels = torch.unique(y).shape[0]\n",
    "task_level = \"graph\" if out_channels == 1 else \"node\"\n",
    "\n",
    "\n",
    "model = Network(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_layers=n_layers,\n",
    "    task_level=task_level,\n",
    ").to(device)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:15:00.301642Z",
     "start_time": "2025-11-17T22:15:00.294836Z"
    }
   },
   "cell_type": "code",
   "source": "print(model)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (base_model): HMPNN(\n",
      "    (linear_node): Linear(in_features=1433, out_features=128, bias=True)\n",
      "    (linear_edge): Linear(in_features=1433, out_features=128, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): HMPNNLayer(\n",
      "        (node_to_hyperedge_messenger): _NodeToHyperedgeMessenger(\n",
      "          (messaging_func): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "        (hyperedge_to_node_messenger): _HyperedgeToNodeMessenger(\n",
      "          (messaging_func): _DefaultHyperedgeToNodeMessagingFunc(\n",
      "            (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (node_batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (hyperedge_batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (updating_func): _DefaultUpdatingFunc()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:15:00.490999Z",
     "start_time": "2025-11-17T22:15:00.482945Z"
    }
   },
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_mask = dataset[\"train_mask\"]\n",
    "val_mask = dataset[\"val_mask\"]\n",
    "test_mask = dataset[\"test_mask\"]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the model, looping over the network for a low amount of epochs. We keep training minimal for the purpose of rapid testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The number of epochs below have been kept low to facilitate debugging and testing. Real use cases should likely require more epochs.**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that:\n",
    "- x_0s: The input feature matrix of the node.\n",
    "- initial_x_1:\n",
    "- incidence_1: The adjacency matrix of the hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:15:26.900280Z",
     "start_time": "2025-11-17T22:15:00.539750Z"
    }
   },
   "source": [
    "torch.manual_seed(0)\n",
    "test_interval = 5\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "initial_x_1 = torch.zeros_like(x_0s)\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x_0s, initial_x_1, incidence_1)\n",
    "    loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss = loss.item()\n",
    "    y_pred = y_hat.argmax(dim=-1)\n",
    "    train_acc = accuracy_score(y[train_mask].cpu(), y_pred[train_mask].cpu())\n",
    "\n",
    "    if epoch % test_interval == 0:\n",
    "        model.eval()\n",
    "\n",
    "        y_hat = model(x_0s, initial_x_1, incidence_1)\n",
    "        val_loss = loss_fn(y_hat[val_mask], y[val_mask]).item()\n",
    "        y_pred = y_hat.argmax(dim=-1)\n",
    "        val_acc = accuracy_score(y[val_mask].cpu(), y_pred[val_mask].cpu())\n",
    "\n",
    "        test_loss = loss_fn(y_hat[test_mask], y[test_mask]).item()\n",
    "        y_pred = y_hat.argmax(dim=-1)\n",
    "        test_acc = accuracy_score(y[test_mask].cpu(), y_pred[test_mask].cpu())\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1} train loss: {train_loss:.4f} train acc: {train_acc:.2f} \"\n",
    "            f\" val loss: {val_loss:.4f} val acc: {val_acc:.2f}\"\n",
    "            f\" test loss: {test_acc:.4f} val acc: {test_acc:.2f}\"\n",
    "        )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 train loss: 1.2665 train acc: 0.82  val loss: 1.9848 val acc: 0.23 test loss: 0.2270 val acc: 0.23\n",
      "Epoch: 11 train loss: 0.8439 train acc: 0.99  val loss: 1.7568 val acc: 0.38 test loss: 0.3970 val acc: 0.40\n",
      "Epoch: 16 train loss: 0.5118 train acc: 1.00  val loss: 1.6843 val acc: 0.40 test loss: 0.4150 val acc: 0.41\n",
      "Epoch: 21 train loss: 0.2717 train acc: 1.00  val loss: 1.5870 val acc: 0.43 test loss: 0.4500 val acc: 0.45\n",
      "Epoch: 26 train loss: 0.1571 train acc: 1.00  val loss: 1.6138 val acc: 0.41 test loss: 0.4220 val acc: 0.42\n",
      "Epoch: 31 train loss: 0.0816 train acc: 1.00  val loss: 1.5894 val acc: 0.45 test loss: 0.4490 val acc: 0.45\n",
      "Epoch: 36 train loss: 0.0478 train acc: 1.00  val loss: 1.6026 val acc: 0.46 test loss: 0.4620 val acc: 0.46\n",
      "Epoch: 41 train loss: 0.0298 train acc: 1.00  val loss: 1.6143 val acc: 0.47 test loss: 0.4670 val acc: 0.47\n",
      "Epoch: 46 train loss: 0.0214 train acc: 1.00  val loss: 1.6487 val acc: 0.47 test loss: 0.4730 val acc: 0.47\n",
      "Epoch: 51 train loss: 0.0160 train acc: 1.00  val loss: 1.6748 val acc: 0.48 test loss: 0.4820 val acc: 0.48\n",
      "Epoch: 56 train loss: 0.0149 train acc: 1.00  val loss: 1.6978 val acc: 0.48 test loss: 0.4900 val acc: 0.49\n",
      "Epoch: 61 train loss: 0.0123 train acc: 1.00  val loss: 1.6882 val acc: 0.47 test loss: 0.4920 val acc: 0.49\n",
      "Epoch: 66 train loss: 0.0097 train acc: 1.00  val loss: 1.6671 val acc: 0.48 test loss: 0.4970 val acc: 0.50\n",
      "Epoch: 71 train loss: 0.0078 train acc: 1.00  val loss: 1.6560 val acc: 0.49 test loss: 0.5020 val acc: 0.50\n",
      "Epoch: 76 train loss: 0.0072 train acc: 1.00  val loss: 1.6504 val acc: 0.49 test loss: 0.5010 val acc: 0.50\n",
      "Epoch: 81 train loss: 0.0066 train acc: 1.00  val loss: 1.6400 val acc: 0.49 test loss: 0.5100 val acc: 0.51\n",
      "Epoch: 86 train loss: 0.0064 train acc: 1.00  val loss: 1.6495 val acc: 0.49 test loss: 0.5100 val acc: 0.51\n",
      "Epoch: 91 train loss: 0.0060 train acc: 1.00  val loss: 1.6724 val acc: 0.50 test loss: 0.5100 val acc: 0.51\n",
      "Epoch: 96 train loss: 0.0051 train acc: 1.00  val loss: 1.6669 val acc: 0.50 test loss: 0.5150 val acc: 0.52\n",
      "Epoch: 101 train loss: 0.0047 train acc: 1.00  val loss: 1.6407 val acc: 0.50 test loss: 0.5190 val acc: 0.52\n",
      "Epoch: 106 train loss: 0.0052 train acc: 1.00  val loss: 1.6398 val acc: 0.50 test loss: 0.5160 val acc: 0.52\n",
      "Epoch: 111 train loss: 0.0047 train acc: 1.00  val loss: 1.6448 val acc: 0.51 test loss: 0.5180 val acc: 0.52\n",
      "Epoch: 116 train loss: 0.0043 train acc: 1.00  val loss: 1.6551 val acc: 0.51 test loss: 0.5180 val acc: 0.52\n",
      "Epoch: 121 train loss: 0.0044 train acc: 1.00  val loss: 1.6511 val acc: 0.51 test loss: 0.5190 val acc: 0.52\n",
      "Epoch: 126 train loss: 0.0037 train acc: 1.00  val loss: 1.6374 val acc: 0.52 test loss: 0.5280 val acc: 0.53\n",
      "Epoch: 131 train loss: 0.0042 train acc: 1.00  val loss: 1.6320 val acc: 0.52 test loss: 0.5340 val acc: 0.53\n",
      "Epoch: 136 train loss: 0.0038 train acc: 1.00  val loss: 1.6340 val acc: 0.52 test loss: 0.5340 val acc: 0.53\n",
      "Epoch: 141 train loss: 0.0032 train acc: 1.00  val loss: 1.6382 val acc: 0.53 test loss: 0.5340 val acc: 0.53\n",
      "Epoch: 146 train loss: 0.0030 train acc: 1.00  val loss: 1.6385 val acc: 0.52 test loss: 0.5350 val acc: 0.54\n",
      "Epoch: 151 train loss: 0.0029 train acc: 1.00  val loss: 1.6402 val acc: 0.52 test loss: 0.5360 val acc: 0.54\n",
      "Epoch: 156 train loss: 0.0030 train acc: 1.00  val loss: 1.6330 val acc: 0.53 test loss: 0.5400 val acc: 0.54\n",
      "Epoch: 161 train loss: 0.0028 train acc: 1.00  val loss: 1.6278 val acc: 0.54 test loss: 0.5430 val acc: 0.54\n",
      "Epoch: 166 train loss: 0.0028 train acc: 1.00  val loss: 1.6297 val acc: 0.53 test loss: 0.5430 val acc: 0.54\n",
      "Epoch: 171 train loss: 0.0025 train acc: 1.00  val loss: 1.6383 val acc: 0.54 test loss: 0.5440 val acc: 0.54\n",
      "Epoch: 176 train loss: 0.0028 train acc: 1.00  val loss: 1.6441 val acc: 0.54 test loss: 0.5420 val acc: 0.54\n",
      "Epoch: 181 train loss: 0.0025 train acc: 1.00  val loss: 1.6333 val acc: 0.53 test loss: 0.5470 val acc: 0.55\n",
      "Epoch: 186 train loss: 0.0023 train acc: 1.00  val loss: 1.6306 val acc: 0.53 test loss: 0.5490 val acc: 0.55\n",
      "Epoch: 191 train loss: 0.0024 train acc: 1.00  val loss: 1.6297 val acc: 0.54 test loss: 0.5490 val acc: 0.55\n",
      "Epoch: 196 train loss: 0.0022 train acc: 1.00  val loss: 1.6327 val acc: 0.54 test loss: 0.5500 val acc: 0.55\n",
      "Epoch: 201 train loss: 0.0022 train acc: 1.00  val loss: 1.6372 val acc: 0.54 test loss: 0.5520 val acc: 0.55\n",
      "Epoch: 206 train loss: 0.0020 train acc: 1.00  val loss: 1.6429 val acc: 0.54 test loss: 0.5530 val acc: 0.55\n",
      "Epoch: 211 train loss: 0.0020 train acc: 1.00  val loss: 1.6474 val acc: 0.53 test loss: 0.5520 val acc: 0.55\n",
      "Epoch: 216 train loss: 0.0019 train acc: 1.00  val loss: 1.6445 val acc: 0.53 test loss: 0.5510 val acc: 0.55\n",
      "Epoch: 221 train loss: 0.0019 train acc: 1.00  val loss: 1.6381 val acc: 0.53 test loss: 0.5530 val acc: 0.55\n",
      "Epoch: 226 train loss: 0.0018 train acc: 1.00  val loss: 1.6241 val acc: 0.54 test loss: 0.5530 val acc: 0.55\n",
      "Epoch: 231 train loss: 0.0019 train acc: 1.00  val loss: 1.6198 val acc: 0.54 test loss: 0.5570 val acc: 0.56\n",
      "Epoch: 236 train loss: 0.0018 train acc: 1.00  val loss: 1.6263 val acc: 0.54 test loss: 0.5560 val acc: 0.56\n",
      "Epoch: 241 train loss: 0.0019 train acc: 1.00  val loss: 1.6353 val acc: 0.54 test loss: 0.5560 val acc: 0.56\n",
      "Epoch: 246 train loss: 0.0016 train acc: 1.00  val loss: 1.6456 val acc: 0.54 test loss: 0.5550 val acc: 0.56\n",
      "Epoch: 251 train loss: 0.0016 train acc: 1.00  val loss: 1.6512 val acc: 0.54 test loss: 0.5560 val acc: 0.56\n",
      "Epoch: 256 train loss: 0.0017 train acc: 1.00  val loss: 1.6552 val acc: 0.54 test loss: 0.5550 val acc: 0.56\n",
      "Epoch: 261 train loss: 0.0017 train acc: 1.00  val loss: 1.6531 val acc: 0.54 test loss: 0.5560 val acc: 0.56\n",
      "Epoch: 266 train loss: 0.0016 train acc: 1.00  val loss: 1.6529 val acc: 0.55 test loss: 0.5560 val acc: 0.56\n",
      "Epoch: 271 train loss: 0.0014 train acc: 1.00  val loss: 1.6493 val acc: 0.55 test loss: 0.5570 val acc: 0.56\n",
      "Epoch: 276 train loss: 0.0013 train acc: 1.00  val loss: 1.6459 val acc: 0.55 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 281 train loss: 0.0016 train acc: 1.00  val loss: 1.6417 val acc: 0.55 test loss: 0.5600 val acc: 0.56\n",
      "Epoch: 286 train loss: 0.0015 train acc: 1.00  val loss: 1.6411 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 291 train loss: 0.0013 train acc: 1.00  val loss: 1.6479 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 296 train loss: 0.0015 train acc: 1.00  val loss: 1.6532 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 301 train loss: 0.0013 train acc: 1.00  val loss: 1.6682 val acc: 0.55 test loss: 0.5610 val acc: 0.56\n",
      "Epoch: 306 train loss: 0.0013 train acc: 1.00  val loss: 1.6860 val acc: 0.55 test loss: 0.5600 val acc: 0.56\n",
      "Epoch: 311 train loss: 0.0011 train acc: 1.00  val loss: 1.6962 val acc: 0.55 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 316 train loss: 0.0012 train acc: 1.00  val loss: 1.6905 val acc: 0.55 test loss: 0.5600 val acc: 0.56\n",
      "Epoch: 321 train loss: 0.0012 train acc: 1.00  val loss: 1.6832 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 326 train loss: 0.0012 train acc: 1.00  val loss: 1.6776 val acc: 0.55 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 331 train loss: 0.0011 train acc: 1.00  val loss: 1.6803 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 336 train loss: 0.0010 train acc: 1.00  val loss: 1.6869 val acc: 0.55 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 341 train loss: 0.0011 train acc: 1.00  val loss: 1.6883 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 346 train loss: 0.0010 train acc: 1.00  val loss: 1.6854 val acc: 0.55 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 351 train loss: 0.0011 train acc: 1.00  val loss: 1.6875 val acc: 0.55 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 356 train loss: 0.0010 train acc: 1.00  val loss: 1.6878 val acc: 0.55 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 361 train loss: 0.0009 train acc: 1.00  val loss: 1.6944 val acc: 0.55 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 366 train loss: 0.0010 train acc: 1.00  val loss: 1.6981 val acc: 0.55 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 371 train loss: 0.0011 train acc: 1.00  val loss: 1.7031 val acc: 0.55 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 376 train loss: 0.0009 train acc: 1.00  val loss: 1.7029 val acc: 0.55 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 381 train loss: 0.0009 train acc: 1.00  val loss: 1.7068 val acc: 0.55 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 386 train loss: 0.0010 train acc: 1.00  val loss: 1.7126 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 391 train loss: 0.0009 train acc: 1.00  val loss: 1.7146 val acc: 0.55 test loss: 0.5610 val acc: 0.56\n",
      "Epoch: 396 train loss: 0.0008 train acc: 1.00  val loss: 1.7174 val acc: 0.55 test loss: 0.5610 val acc: 0.56\n",
      "Epoch: 401 train loss: 0.0009 train acc: 1.00  val loss: 1.7203 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 406 train loss: 0.0009 train acc: 1.00  val loss: 1.7293 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 411 train loss: 0.0008 train acc: 1.00  val loss: 1.7359 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 416 train loss: 0.0009 train acc: 1.00  val loss: 1.7345 val acc: 0.55 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 421 train loss: 0.0009 train acc: 1.00  val loss: 1.7358 val acc: 0.55 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 426 train loss: 0.0007 train acc: 1.00  val loss: 1.7313 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 431 train loss: 0.0007 train acc: 1.00  val loss: 1.7304 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 436 train loss: 0.0008 train acc: 1.00  val loss: 1.7284 val acc: 0.55 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 441 train loss: 0.0007 train acc: 1.00  val loss: 1.7287 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 446 train loss: 0.0007 train acc: 1.00  val loss: 1.7315 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 451 train loss: 0.0009 train acc: 1.00  val loss: 1.7324 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 456 train loss: 0.0007 train acc: 1.00  val loss: 1.7339 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 461 train loss: 0.0007 train acc: 1.00  val loss: 1.7386 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 466 train loss: 0.0007 train acc: 1.00  val loss: 1.7426 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 471 train loss: 0.0007 train acc: 1.00  val loss: 1.7443 val acc: 0.55 test loss: 0.5610 val acc: 0.56\n",
      "Epoch: 476 train loss: 0.0007 train acc: 1.00  val loss: 1.7447 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 481 train loss: 0.0007 train acc: 1.00  val loss: 1.7410 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 486 train loss: 0.0006 train acc: 1.00  val loss: 1.7409 val acc: 0.55 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 491 train loss: 0.0006 train acc: 1.00  val loss: 1.7493 val acc: 0.55 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 496 train loss: 0.0006 train acc: 1.00  val loss: 1.7552 val acc: 0.55 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 501 train loss: 0.0006 train acc: 1.00  val loss: 1.7588 val acc: 0.55 test loss: 0.5580 val acc: 0.56\n",
      "Epoch: 506 train loss: 0.0006 train acc: 1.00  val loss: 1.7621 val acc: 0.55 test loss: 0.5580 val acc: 0.56\n",
      "Epoch: 511 train loss: 0.0006 train acc: 1.00  val loss: 1.7642 val acc: 0.55 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 516 train loss: 0.0006 train acc: 1.00  val loss: 1.7651 val acc: 0.55 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 521 train loss: 0.0006 train acc: 1.00  val loss: 1.7649 val acc: 0.55 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 526 train loss: 0.0006 train acc: 1.00  val loss: 1.7623 val acc: 0.56 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 531 train loss: 0.0007 train acc: 1.00  val loss: 1.7586 val acc: 0.56 test loss: 0.5600 val acc: 0.56\n",
      "Epoch: 536 train loss: 0.0006 train acc: 1.00  val loss: 1.7581 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 541 train loss: 0.0006 train acc: 1.00  val loss: 1.7613 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 546 train loss: 0.0005 train acc: 1.00  val loss: 1.7695 val acc: 0.56 test loss: 0.5610 val acc: 0.56\n",
      "Epoch: 551 train loss: 0.0005 train acc: 1.00  val loss: 1.7767 val acc: 0.56 test loss: 0.5610 val acc: 0.56\n",
      "Epoch: 556 train loss: 0.0006 train acc: 1.00  val loss: 1.7806 val acc: 0.56 test loss: 0.5600 val acc: 0.56\n",
      "Epoch: 561 train loss: 0.0005 train acc: 1.00  val loss: 1.7803 val acc: 0.56 test loss: 0.5600 val acc: 0.56\n",
      "Epoch: 566 train loss: 0.0006 train acc: 1.00  val loss: 1.7795 val acc: 0.56 test loss: 0.5610 val acc: 0.56\n",
      "Epoch: 571 train loss: 0.0006 train acc: 1.00  val loss: 1.7774 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 576 train loss: 0.0005 train acc: 1.00  val loss: 1.7791 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 581 train loss: 0.0005 train acc: 1.00  val loss: 1.7793 val acc: 0.56 test loss: 0.5610 val acc: 0.56\n",
      "Epoch: 586 train loss: 0.0005 train acc: 1.00  val loss: 1.7814 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 591 train loss: 0.0005 train acc: 1.00  val loss: 1.7853 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 596 train loss: 0.0005 train acc: 1.00  val loss: 1.7871 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 601 train loss: 0.0005 train acc: 1.00  val loss: 1.7895 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 606 train loss: 0.0005 train acc: 1.00  val loss: 1.7925 val acc: 0.56 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 611 train loss: 0.0004 train acc: 1.00  val loss: 1.7977 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 616 train loss: 0.0005 train acc: 1.00  val loss: 1.7983 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 621 train loss: 0.0005 train acc: 1.00  val loss: 1.7972 val acc: 0.56 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 626 train loss: 0.0005 train acc: 1.00  val loss: 1.7972 val acc: 0.56 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 631 train loss: 0.0004 train acc: 1.00  val loss: 1.8005 val acc: 0.57 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 636 train loss: 0.0005 train acc: 1.00  val loss: 1.8036 val acc: 0.57 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 641 train loss: 0.0005 train acc: 1.00  val loss: 1.8032 val acc: 0.56 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 646 train loss: 0.0004 train acc: 1.00  val loss: 1.8023 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 651 train loss: 0.0005 train acc: 1.00  val loss: 1.8021 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 656 train loss: 0.0004 train acc: 1.00  val loss: 1.8042 val acc: 0.56 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 661 train loss: 0.0004 train acc: 1.00  val loss: 1.8054 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 666 train loss: 0.0004 train acc: 1.00  val loss: 1.8039 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 671 train loss: 0.0005 train acc: 1.00  val loss: 1.8048 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 676 train loss: 0.0004 train acc: 1.00  val loss: 1.8111 val acc: 0.56 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 681 train loss: 0.0004 train acc: 1.00  val loss: 1.8175 val acc: 0.56 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 686 train loss: 0.0004 train acc: 1.00  val loss: 1.8208 val acc: 0.56 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 691 train loss: 0.0004 train acc: 1.00  val loss: 1.8260 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 696 train loss: 0.0004 train acc: 1.00  val loss: 1.8269 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 701 train loss: 0.0004 train acc: 1.00  val loss: 1.8270 val acc: 0.56 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 706 train loss: 0.0004 train acc: 1.00  val loss: 1.8297 val acc: 0.56 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 711 train loss: 0.0004 train acc: 1.00  val loss: 1.8270 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 716 train loss: 0.0004 train acc: 1.00  val loss: 1.8211 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 721 train loss: 0.0004 train acc: 1.00  val loss: 1.8199 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 726 train loss: 0.0004 train acc: 1.00  val loss: 1.8231 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 731 train loss: 0.0004 train acc: 1.00  val loss: 1.8246 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 736 train loss: 0.0004 train acc: 1.00  val loss: 1.8261 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 741 train loss: 0.0004 train acc: 1.00  val loss: 1.8296 val acc: 0.56 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 746 train loss: 0.0004 train acc: 1.00  val loss: 1.8370 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 751 train loss: 0.0003 train acc: 1.00  val loss: 1.8443 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 756 train loss: 0.0004 train acc: 1.00  val loss: 1.8507 val acc: 0.56 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 761 train loss: 0.0004 train acc: 1.00  val loss: 1.8538 val acc: 0.56 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 766 train loss: 0.0004 train acc: 1.00  val loss: 1.8552 val acc: 0.56 test loss: 0.5590 val acc: 0.56\n",
      "Epoch: 771 train loss: 0.0003 train acc: 1.00  val loss: 1.8521 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 776 train loss: 0.0003 train acc: 1.00  val loss: 1.8508 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 781 train loss: 0.0003 train acc: 1.00  val loss: 1.8493 val acc: 0.56 test loss: 0.5620 val acc: 0.56\n",
      "Epoch: 786 train loss: 0.0004 train acc: 1.00  val loss: 1.8486 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 791 train loss: 0.0003 train acc: 1.00  val loss: 1.8479 val acc: 0.57 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 796 train loss: 0.0004 train acc: 1.00  val loss: 1.8507 val acc: 0.56 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 801 train loss: 0.0003 train acc: 1.00  val loss: 1.8542 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 806 train loss: 0.0003 train acc: 1.00  val loss: 1.8557 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 811 train loss: 0.0003 train acc: 1.00  val loss: 1.8583 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 816 train loss: 0.0003 train acc: 1.00  val loss: 1.8613 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 821 train loss: 0.0003 train acc: 1.00  val loss: 1.8630 val acc: 0.56 test loss: 0.5630 val acc: 0.56\n",
      "Epoch: 826 train loss: 0.0003 train acc: 1.00  val loss: 1.8628 val acc: 0.56 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 831 train loss: 0.0003 train acc: 1.00  val loss: 1.8620 val acc: 0.57 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 836 train loss: 0.0003 train acc: 1.00  val loss: 1.8615 val acc: 0.57 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 841 train loss: 0.0003 train acc: 1.00  val loss: 1.8598 val acc: 0.57 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 846 train loss: 0.0003 train acc: 1.00  val loss: 1.8586 val acc: 0.57 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 851 train loss: 0.0003 train acc: 1.00  val loss: 1.8588 val acc: 0.57 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 856 train loss: 0.0003 train acc: 1.00  val loss: 1.8611 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 861 train loss: 0.0003 train acc: 1.00  val loss: 1.8641 val acc: 0.57 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 866 train loss: 0.0003 train acc: 1.00  val loss: 1.8661 val acc: 0.56 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 871 train loss: 0.0003 train acc: 1.00  val loss: 1.8659 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 876 train loss: 0.0003 train acc: 1.00  val loss: 1.8706 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 881 train loss: 0.0003 train acc: 1.00  val loss: 1.8762 val acc: 0.57 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 886 train loss: 0.0003 train acc: 1.00  val loss: 1.8763 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 891 train loss: 0.0003 train acc: 1.00  val loss: 1.8787 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 896 train loss: 0.0003 train acc: 1.00  val loss: 1.8829 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 901 train loss: 0.0003 train acc: 1.00  val loss: 1.8836 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 906 train loss: 0.0003 train acc: 1.00  val loss: 1.8846 val acc: 0.57 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 911 train loss: 0.0003 train acc: 1.00  val loss: 1.8834 val acc: 0.57 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 916 train loss: 0.0003 train acc: 1.00  val loss: 1.8801 val acc: 0.57 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 921 train loss: 0.0003 train acc: 1.00  val loss: 1.8787 val acc: 0.57 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 926 train loss: 0.0003 train acc: 1.00  val loss: 1.8772 val acc: 0.57 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 931 train loss: 0.0003 train acc: 1.00  val loss: 1.8806 val acc: 0.57 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 936 train loss: 0.0003 train acc: 1.00  val loss: 1.8851 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 941 train loss: 0.0003 train acc: 1.00  val loss: 1.8878 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 946 train loss: 0.0003 train acc: 1.00  val loss: 1.8901 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 951 train loss: 0.0003 train acc: 1.00  val loss: 1.8943 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 956 train loss: 0.0003 train acc: 1.00  val loss: 1.8985 val acc: 0.56 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 961 train loss: 0.0002 train acc: 1.00  val loss: 1.9009 val acc: 0.56 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 966 train loss: 0.0002 train acc: 1.00  val loss: 1.9010 val acc: 0.56 test loss: 0.5640 val acc: 0.56\n",
      "Epoch: 971 train loss: 0.0003 train acc: 1.00  val loss: 1.9007 val acc: 0.57 test loss: 0.5650 val acc: 0.56\n",
      "Epoch: 976 train loss: 0.0002 train acc: 1.00  val loss: 1.9010 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 981 train loss: 0.0002 train acc: 1.00  val loss: 1.8990 val acc: 0.57 test loss: 0.5660 val acc: 0.57\n",
      "Epoch: 986 train loss: 0.0002 train acc: 1.00  val loss: 1.8969 val acc: 0.57 test loss: 0.5670 val acc: 0.57\n",
      "Epoch: 991 train loss: 0.0002 train acc: 1.00  val loss: 1.8970 val acc: 0.57 test loss: 0.5670 val acc: 0.57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      9\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 10\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_0s\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_x_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mincidence_1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(y_hat[train_mask], y[train_mask])\n\u001B[1;32m     12\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/miniconda3/envs/topox/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/topox/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[8], line 40\u001B[0m, in \u001B[0;36mNetwork.forward\u001B[0;34m(self, x_0, x_1, incidence_1)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_0, x_1, incidence_1):\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;66;03m# Base model\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m     x_0, x_1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mincidence_1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;66;03m# Pool over all nodes in the hypergraph\u001B[39;00m\n\u001B[1;32m     43\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(x_0, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_pool \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m x_0\n",
      "File \u001B[0;32m~/miniconda3/envs/topox/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/topox/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/topomodelx/topomodelx/nn/hypergraph/hmpnn.py:85\u001B[0m, in \u001B[0;36mHMPNN.forward\u001B[0;34m(self, x_0, x_1, incidence_1)\u001B[0m\n\u001B[1;32m     82\u001B[0m x_1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_edge(x_1)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 85\u001B[0m     x_0, x_1 \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mincidence_1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x_0, x_1\n",
      "File \u001B[0;32m~/miniconda3/envs/topox/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/topox/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/topomodelx/topomodelx/nn/hypergraph/hmpnn_layer.py:374\u001B[0m, in \u001B[0;36mHMPNNLayer.forward\u001B[0;34m(self, x_0, x_1, incidence_1)\u001B[0m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_0, x_1, incidence_1):\n\u001B[1;32m    356\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Forward computation.\u001B[39;00m\n\u001B[1;32m    357\u001B[0m \n\u001B[1;32m    358\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;124;03m        Output features of the hyperedges.\u001B[39;00m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 374\u001B[0m     node_messages_aggregated, node_messages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode_to_hyperedge_messenger\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx_0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mincidence_1\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    377\u001B[0m     hyperedge_messages_aggregated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhyperedge_to_node_messenger(\n\u001B[1;32m    378\u001B[0m         x_1, incidence_1, node_messages\n\u001B[1;32m    379\u001B[0m     )\n\u001B[1;32m    381\u001B[0m     x_0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdating_func(\n\u001B[1;32m    382\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_regular_dropout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_batchnorm(x_0)),\n\u001B[1;32m    383\u001B[0m         hyperedge_messages_aggregated,\n\u001B[1;32m    384\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/topox/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/topox/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/topomodelx/topomodelx/nn/hypergraph/hmpnn_layer.py:101\u001B[0m, in \u001B[0;36m_NodeToHyperedgeMessenger.forward\u001B[0;34m(self, x_source, neighborhood)\u001B[0m\n\u001B[1;32m     98\u001B[0m source_index_j, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_index_i \u001B[38;5;241m=\u001B[39m neighborhood\u001B[38;5;241m.\u001B[39mindices()\n\u001B[1;32m    100\u001B[0m x_message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmessage(x_source)\n\u001B[0;32m--> 101\u001B[0m x_message_aggregated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maggregate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_message\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_select\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_index_j\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x_message_aggregated, x_message\n",
      "File \u001B[0;32m~/PycharmProjects/topomodelx/topomodelx/base/message_passing.py:190\u001B[0m, in \u001B[0;36mMessagePassing.aggregate\u001B[0;34m(self, x_message)\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Aggregate messages on each target cell.\u001B[39;00m\n\u001B[1;32m    168\u001B[0m \n\u001B[1;32m    169\u001B[0m \u001B[38;5;124;03mA target cell receives messages from several source cells.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;124;03m    Assumes that all target cells have the same rank s.\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    189\u001B[0m aggr \u001B[38;5;241m=\u001B[39m scatter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggr_func)\n\u001B[0;32m--> 190\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43maggr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_message\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_index_i\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/topomodelx/topomodelx/utils/scatter.py:38\u001B[0m, in \u001B[0;36mscatter_sum\u001B[0;34m(src, index, dim, out, dim_size)\u001B[0m\n\u001B[1;32m     36\u001B[0m     size[dim] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m     size[dim] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     39\u001B[0m out \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(size, dtype\u001B[38;5;241m=\u001B[39msrc\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39msrc\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\u001B[38;5;241m.\u001B[39mscatter_add_(dim, index, src)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:26:34.068633Z",
     "start_time": "2025-11-17T22:26:33.987862Z"
    }
   },
   "source": "print(y_hat[train_mask])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3404, -2.3616, -1.0914, 13.5829, -1.2837, -2.1233, -1.9325],\n",
      "        [-3.2887, -2.6952, -2.7585, -3.0699, 12.0892, -1.7336, -2.0955],\n",
      "        [-2.7816, -3.1118, -2.4088, -3.2886, 11.7967, -2.2970, -1.7538],\n",
      "        [11.3011, -2.4660, -2.1042, -3.9745, -2.9593, -2.6895, -2.0052],\n",
      "        [-3.0607, -2.2961, -0.9514, 14.2813, -1.5308, -1.7687, -1.6491],\n",
      "        [-1.7914, -2.5848, 13.5673, -2.3254, -1.9857, -2.4514, -1.8735],\n",
      "        [11.6065, -2.2457, -2.4827, -4.0519, -3.0297, -2.2605, -2.6045],\n",
      "        [-2.7894, -2.1635, -1.1661, 14.3403, -1.4402, -1.7256, -2.1138],\n",
      "        [-2.4734, -2.5382, -1.2406, 14.4546, -1.3764, -1.7861, -2.2584],\n",
      "        [-1.7141, -2.4663, 11.6857, -2.3136, -1.2823, -2.0484, -1.6935],\n",
      "        [11.5254, -2.1919, -2.1277, -3.6737, -3.4718, -2.4170, -2.3764],\n",
      "        [11.7715, -2.3391, -2.1153, -3.8919, -3.3441, -2.6664, -2.2455],\n",
      "        [-3.0241, -2.9473, -2.3750, -3.3107, 11.6597, -2.1437, -1.5873],\n",
      "        [-2.5554, -2.3240, -1.4496, 14.2534, -1.3987, -2.1169, -1.6800],\n",
      "        [-2.6914, -2.3710, -1.0586, 14.5739, -1.4816, -2.1797, -1.8811],\n",
      "        [-2.0654, -2.5663, -1.2635, 14.0233, -1.1760, -2.3511, -2.2558],\n",
      "        [-1.6343, -2.7382, 13.6973, -2.6161, -1.8228, -2.5801, -1.8775],\n",
      "        [-2.3594, -2.2297, -1.2330, 14.1809, -1.5551, -2.1672, -2.0154],\n",
      "        [-0.7542, 12.6664, -2.0516, -2.6115, -1.5343, -0.8539, -2.9951],\n",
      "        [-2.7719, -2.1414, -1.2633, 14.7850, -1.4589, -2.0413, -2.1643],\n",
      "        [-2.1795, -2.5326, -2.0974, -2.4496, -1.0965, 13.7334, -3.3036],\n",
      "        [-2.7429, -2.3719, -1.2657, 14.4200, -1.3841, -1.8253, -1.9579],\n",
      "        [-3.0633, -3.3411, -2.5609, -2.9239, 12.1838, -2.0144, -2.0768],\n",
      "        [-2.0390, -3.1760, -1.8846, -2.2494, -0.7961, -2.9164, 12.2304],\n",
      "        [-2.5691, -2.4821, -1.0594, 14.5437, -1.7370, -2.0588, -1.7140],\n",
      "        [-2.0385, -3.1269, -1.8795, 13.7058, -1.1357, -1.2472, -2.4236],\n",
      "        [-2.1616, -3.6339, -2.2035, -2.7988, -0.8964, -3.2668, 14.1580],\n",
      "        [-2.4404, -2.5430, -1.4470, 14.1669, -1.4056, -1.9298, -1.7701],\n",
      "        [-1.6381, -2.6472, 13.3998, -2.3801, -2.0180, -2.1944, -1.9526],\n",
      "        [-2.7734, -2.4847, -1.8142, -2.4138,  9.8108, -1.8485, -1.7779],\n",
      "        [-2.7716, -2.7611, -0.9225, 14.1108, -1.2374, -1.8762, -1.7397],\n",
      "        [-2.4233, -3.3353, -1.8886, -2.4756, -0.9096, -3.1295, 13.5812],\n",
      "        [11.6215, -2.2836, -2.4540, -4.0607, -3.1644, -2.6205, -2.0019],\n",
      "        [-2.9850, -3.1332, -2.5647, -3.3441, 12.0873, -2.2245, -1.6967],\n",
      "        [-1.8911, -2.6493, 13.8171, -2.3474, -1.9877, -2.2015, -2.0857],\n",
      "        [11.7039, -2.1899, -1.9551, -3.9544, -3.4286, -2.6905, -2.2846],\n",
      "        [-1.2652, 14.1592, -2.0522, -2.9149, -1.7830, -1.3684, -2.4800],\n",
      "        [-1.9789, -2.4971, -1.8119, -2.7928, -1.2700, 13.6836, -3.1534],\n",
      "        [-3.2207, -2.8065, -2.7436, -2.9315, 12.0050, -1.9831, -1.9046],\n",
      "        [-3.0760, -3.0587, -2.7660, -3.1137, 12.3332, -2.1404, -1.9127],\n",
      "        [-2.6975, -2.2993, -1.2084, 14.6929, -1.4583, -1.9855, -2.1189],\n",
      "        [-2.1822, -3.8147, -2.0852, -2.4140, -0.9480, -3.2951, 14.0167],\n",
      "        [-2.3410, -3.6672, -2.0219, -2.7855, -0.9811, -3.0012, 14.1633],\n",
      "        [-3.1405, -2.9776, -2.6827, -3.0373, 12.0887, -2.1580, -1.7506],\n",
      "        [-2.8840, -2.5955, -1.2680, 14.2016, -1.0247, -1.9426, -1.7542],\n",
      "        [-2.5965, -2.5594, -1.1402, 13.8594, -1.0786, -2.0405, -1.9061],\n",
      "        [-1.7942, -2.6560, 12.7818, -2.2493, -1.6681, -2.2286, -1.8379],\n",
      "        [-1.7633, -2.6744, -1.8353, -2.7070, -1.2683, 13.5400, -3.3338],\n",
      "        [-2.9910, -2.7650, -1.2405, 14.1443, -1.1369, -1.7289, -1.5912],\n",
      "        [-3.2789, -2.7083, -2.8669, -2.9238, 12.2048, -1.9233, -2.1160],\n",
      "        [-2.0579, -1.9748, -2.3050, -2.3862, -1.1451, 13.3784, -3.3350],\n",
      "        [-2.5495, -2.6862, -1.3652, 14.2859, -1.3006, -1.8176, -1.8883],\n",
      "        [10.3573, -1.8711, -1.9888, -3.6969, -2.8675, -2.5170, -2.0743],\n",
      "        [-1.7926, -2.7492, 13.7372, -2.3680, -2.0080, -2.2728, -1.9727],\n",
      "        [-1.2400, 12.3947, -1.5901, -2.3427, -1.3552, -1.2766, -2.5892],\n",
      "        [-2.9545, -3.1697, -2.5917, -3.2710, 11.9842, -2.2212, -1.6945],\n",
      "        [-2.0764, -3.6725, -1.9492, -2.5900, -1.0043, -3.2869, 13.8097],\n",
      "        [-2.7213, -2.1590, -1.3888, 14.5832, -1.5016, -1.8851, -2.0295],\n",
      "        [-1.9048, -2.4746, 12.2805, -2.2718, -1.3656, -2.0230, -1.8813],\n",
      "        [-1.7690, -2.6891, 13.7381, -2.5065, -1.8063, -2.3196, -2.0668],\n",
      "        [11.5500, -2.4775, -2.0799, -3.6455, -3.4489, -2.1986, -2.5230],\n",
      "        [11.3138, -2.2635, -2.2610, -3.6683, -3.0878, -2.6083, -2.2800],\n",
      "        [11.1359, -2.0662, -2.0410, -4.0199, -3.0691, -2.3019, -2.3408],\n",
      "        [-3.2501, -2.7683, -2.7380, -2.8014, 11.9936, -1.7842, -2.1917],\n",
      "        [-1.6955, -2.8415, 13.7421, -2.3931, -1.9431, -2.5051, -1.9451],\n",
      "        [11.3232, -2.3733, -2.4156, -3.7181, -2.8483, -2.4535, -2.4769],\n",
      "        [-3.1837, -2.8716, -2.2666, -3.1239, 11.7180, -1.9106, -2.0113],\n",
      "        [-2.0863, -2.2953, -2.3871, -2.6551, -1.2225, 14.2044, -3.4312],\n",
      "        [-1.9329, -2.7390, 13.9618, -2.4558, -1.7977, -2.2803, -2.1536],\n",
      "        [-1.8559, -3.8403, -1.9755, -2.3537, -0.9752, -3.1989, 13.3293],\n",
      "        [-2.0012, -2.4828, -2.1494, -2.8243, -1.1996, 13.8665, -3.1648],\n",
      "        [-1.9123, -2.6083, 13.9152, -2.3576, -1.7185, -2.6067, -2.1527],\n",
      "        [-1.6508, -2.5983, 12.7095, -2.2747, -1.8028, -2.0696, -1.9574],\n",
      "        [-1.8930, -2.9785, 13.8193, -2.7304, -1.6233, -2.2425, -1.8771],\n",
      "        [11.1227, -2.1231, -2.3666, -3.7079, -2.9472, -2.5758, -2.2229],\n",
      "        [-3.0489, -3.1301, -2.8149, -3.2315, 12.2923, -2.2489, -1.7512],\n",
      "        [-2.0883, -2.4585, -2.2483, -2.6573, -1.0995, 13.8951, -3.1953],\n",
      "        [-2.2017, -3.6914, -2.1294, -2.4995, -1.1076, -3.0680, 14.0403],\n",
      "        [-3.3303, -2.9161, -2.6464, -3.0057, 12.2771, -1.9154, -2.0482],\n",
      "        [11.6040, -2.2241, -2.1113, -3.8677, -3.3317, -2.4652, -2.4314],\n",
      "        [11.6405, -2.2765, -2.4720, -3.9772, -3.0488, -2.6186, -2.1997],\n",
      "        [11.7137, -2.3674, -2.3454, -3.9055, -3.1304, -2.7668, -2.1871],\n",
      "        [-2.9869, -2.9411, -2.2228, -2.7431, 11.2976, -1.9529, -2.0083],\n",
      "        [-1.9575, -2.8341, 14.2869, -2.4553, -1.9679, -2.3700, -2.0245],\n",
      "        [-3.0453, -2.9861, -2.8158, -3.1023, 12.2599, -2.1872, -1.8842],\n",
      "        [-1.2182, 13.9859, -1.7729, -2.4011, -1.7622, -1.4119, -3.0814],\n",
      "        [-2.9036, -2.9781, -2.6236, -3.1053, 11.8234, -2.1157, -1.8791],\n",
      "        [-2.0299, -3.7584, -1.8910, -2.3158, -1.1572, -3.0855, 13.7726],\n",
      "        [11.3169, -2.4251, -2.5364, -3.8432, -3.0573, -2.2419, -2.1801],\n",
      "        [-2.9728, -3.1067, -2.6958, -3.2947, 12.1399, -2.2505, -1.6239],\n",
      "        [-1.8311, -2.8370, 14.0619, -2.5490, -1.8792, -2.3499, -2.0476],\n",
      "        [-2.9747, -3.2020, -2.7422, -3.0939, 12.2798, -2.3180, -1.8260],\n",
      "        [-2.2317, -3.5209, -1.8556, -2.6817, -0.9321, -3.1579, 13.7377],\n",
      "        [-1.6646, -3.5231, -2.2447, -2.6588, -0.5949, -2.6363, 11.9744],\n",
      "        [11.6796, -2.2142, -2.3740, -3.9742, -3.0750, -2.8115, -2.2197],\n",
      "        [11.5404, -2.2169, -2.5275, -3.9536, -2.9984, -2.6588, -2.1524],\n",
      "        [-2.4025, -3.6192, -1.8611, -2.5438, -0.8840, -3.2478, 14.0159],\n",
      "        [-1.8109, -2.3979, -2.0038, -2.9950, -1.1949, 13.5074, -3.1394],\n",
      "        [11.5460, -2.2617, -2.2679, -3.9083, -3.0774, -2.5049, -2.4172],\n",
      "        [-2.1843, -3.8904, -1.8977, -2.4758, -0.8717, -3.3682, 13.9893],\n",
      "        [11.5789, -2.2693, -2.0880, -3.9819, -3.1963, -2.6042, -2.3174],\n",
      "        [-1.8945, -2.6112, 13.8795, -2.4850, -1.9369, -2.2959, -2.0654],\n",
      "        [-1.0547, 13.6468, -1.7152, -2.7743, -1.5297, -1.5476, -2.8592],\n",
      "        [-1.5456, 13.8052, -2.3347, -2.4599, -1.0907, -1.4007, -2.7047],\n",
      "        [-1.2040, 14.1945, -1.8216, -2.4370, -1.7184, -1.5981, -2.9862],\n",
      "        [-1.6458, -2.6990, 13.6273, -2.3918, -1.9442, -2.4908, -2.0188],\n",
      "        [-2.3172, -3.4312, -1.9299, -2.8136, -1.0566, -2.8422, 13.8043],\n",
      "        [-2.0845, -2.3868, -1.9234, -2.7730, -1.1620, 13.8958, -3.3695],\n",
      "        [-1.8630, -3.9966, -2.0144, -2.1354, -1.2327, -3.6013, 13.8606],\n",
      "        [-1.5465, 13.1545, -1.7032, -2.7063, -1.4207, -1.0252, -2.6440],\n",
      "        [-1.7162, -2.8076, 13.6836, -2.4004, -1.8101, -2.4104, -1.9896],\n",
      "        [-1.4868, -2.6767, 12.1559, -2.4879, -1.3269, -2.1978, -1.9770],\n",
      "        [-0.8911, 14.1629, -1.7150, -2.5732, -1.8992, -1.7090, -3.0245],\n",
      "        [-1.9375, -2.4666, -2.0086, -2.8813, -1.3983, 13.6884, -2.9503],\n",
      "        [-2.0060, -2.2640, -2.0663, -2.5899, -1.3343, 13.7646, -3.2819],\n",
      "        [-2.0232, -2.2805, -2.4109, -2.2371, -1.0064, 13.4087, -3.3217],\n",
      "        [-2.0072, -3.5462, -1.9581, -2.6066, -1.2591, -2.9571, 13.6686],\n",
      "        [-2.0508, -1.9138, -2.3298, -2.7446, -0.9877, 13.4879, -3.3180],\n",
      "        [-2.3027, -3.8673, -1.9594, -2.5314, -0.7463, -3.3152, 14.0639],\n",
      "        [-2.0551, -2.6683, -2.1562, -2.6258, -1.0948, 13.7396, -3.1106],\n",
      "        [-1.7424, -2.0866, -2.1209, -2.7442, -1.0290, 12.3853, -2.7885],\n",
      "        [-1.0948, 14.1850, -2.0687, -2.8791, -1.6497, -1.2833, -2.9021],\n",
      "        [-1.9934, -3.5720, -2.0124, -2.5349, -1.1401, -3.2269, 13.7064],\n",
      "        [-1.8064, -3.6954, -2.0470, -2.2363, -1.0766, -3.6065, 13.6678],\n",
      "        [-0.9649, 14.4000, -1.5843, -2.2930, -2.1794, -1.4457, -3.1685],\n",
      "        [-1.9383, -2.3724, -2.1599, -2.8181, -1.2439, 13.7027, -3.0815],\n",
      "        [-1.0789, 13.5398, -1.8922, -2.7029, -1.3263, -1.5491, -2.9040],\n",
      "        [-2.1754, -3.6622, -1.9717, -2.5708, -0.9859, -3.2859, 13.9516],\n",
      "        [-1.8723, -2.4980, -2.1247, -2.7317, -1.2474, 13.7746, -3.2440],\n",
      "        [-1.8681, -2.4384, -2.2111, -2.9469, -1.3693, 13.4829, -2.6950],\n",
      "        [-2.0654, -2.3612, -2.2790, -2.6550, -1.1863, 14.0478, -3.3817],\n",
      "        [-1.3789, 14.0709, -1.7941, -2.0768, -1.4055, -1.6668, -3.2933],\n",
      "        [-1.6319, -2.7882, -1.8841, -2.5368, -1.3200, 13.5651, -3.4261],\n",
      "        [-1.2581, 13.9898, -1.8435, -2.6068, -1.7327, -1.3250, -2.8900],\n",
      "        [-1.5662, 14.2357, -2.1735, -2.2316, -1.6090, -1.0852, -3.0183],\n",
      "        [-1.0438, 13.9053, -1.8183, -2.4664, -1.8225, -1.6218, -2.8173],\n",
      "        [-1.5520, 14.7638, -2.1495, -2.2664, -1.8128, -1.2619, -3.0318],\n",
      "        [-1.0587, 13.9583, -2.3969, -2.5370, -1.3524, -1.0820, -3.1788],\n",
      "        [-1.3724, 13.9798, -1.9406, -2.6557, -1.7993, -1.3469, -2.4333],\n",
      "        [-1.3135, 14.4234, -2.1337, -2.8604, -1.4990, -1.2156, -2.9656]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T22:18:10.038103Z",
     "start_time": "2025-11-17T22:18:10.011159Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_mask)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  ..., False, False, False], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
